#!/usr/bin/env python3
"""
rag.py
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
RAG minimal (FAISS + OpenAI) adaptÃ© projet Assistant PÃ©dagogique (RQ2).

âœ… PDF + TXT ingestion
âœ… Chunking (RecursiveCharacterTextSplitter)
âœ… FAISS vector search persistant
âœ… MÃ©tadonnÃ©es par chunk (source, page, type)
âœ… RÃ©ponses avec citations + garde-fou â€œje ne sais pasâ€
âœ… Historique conversationnel lÃ©ger

DÃ©pendances
-----------
pip install openai==1.* faiss-cpu numpy pypdf langchain-text-splitters tqdm python-dotenv
"""

from __future__ import annotations
import os, json, textwrap, hashlib
from dataclasses import dataclass
from pathlib import Path
from typing import Any, Dict, List, Tuple

import faiss
import numpy as np
import openai

from langchain_text_splitters import RecursiveCharacterTextSplitter
from pypdf import PdfReader
from tqdm.auto import tqdm
from dotenv import load_dotenv

load_dotenv()

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Config â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
BASE_DIR = Path(__file__).resolve().parent

# âœ… Mets tes PDFs didactiques / cours / fiches ici :
DOCS_DIR = Path(os.getenv("DOCS_DIR", str(BASE_DIR / "data" / "corpus")))

# âœ… Dossier persistant pour lâ€™index
VSTORE_DIR = Path(os.getenv("VSTORE_DIR", str(BASE_DIR / "vectorstore")))
VSTORE_DIR.mkdir(parents=True, exist_ok=True)
FAISS_PATH = VSTORE_DIR / "faiss.index"
META_PATH = VSTORE_DIR / "faiss.meta.json"

EMBED_MODEL = os.getenv("EMBED_MODEL", "text-embedding-3-small")
CHAT_MODEL = os.getenv("CHAT_MODEL", "gpt-4o-mini")

CHUNK_SIZE = int(os.getenv("CHUNK_SIZE", "900"))
CHUNK_OVERLAP = int(os.getenv("CHUNK_OVERLAP", "180"))
TOP_K = int(os.getenv("TOP_K", "8"))

SYSTEM_PROMPT = (
    "Tu es un assistant pÃ©dagogique de mathÃ©matiques, prÃ©cis et utile.\n"
    "RÃ¨gles:\n"
    "1) RÃ©ponds UNIQUEMENT Ã  partir du contexte fourni.\n"
    "2) Si lâ€™information nâ€™est pas dans le contexte, dis clairement: Â« Je ne sais pas. Â»\n"
    "3) Donne une rÃ©ponse structurÃ©e: explication courte + exemple si pertinent.\n"
    "4) Termine par des citations de sources sous la forme: [source:page].\n"
    "5) Ne fabrique jamais de rÃ©fÃ©rences.\n"
)

openai.api_key = os.getenv("OPENAI_API_KEY")
assert openai.api_key, "ğŸ‘‰  Please set OPENAI_API_KEY in your environment!"

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Data model â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@dataclass
class Chunk:
    text: str
    meta: Dict[str, Any]  # {"source": "...", "page": 3, "type": "pdf", ...}

def _stable_id(s: str) -> str:
    return hashlib.sha1(s.encode("utf-8", errors="ignore")).hexdigest()[:16]

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Ingestion â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def read_pdf_pages(path: Path) -> List[Tuple[int, str]]:
    """Return list of (page_index_1based, text)."""
    reader = PdfReader(str(path))
    pages = []
    for i, page in enumerate(reader.pages, start=1):
        pages.append((i, page.extract_text() or ""))
    return pages

def load_and_split() -> List[Chunk]:
    if not DOCS_DIR.exists():
        raise RuntimeError(f"DOCS_DIR introuvable: {DOCS_DIR} (crÃ©e-le et ajoute des PDF/TXT)")

    splitter = RecursiveCharacterTextSplitter(
        chunk_size=CHUNK_SIZE, chunk_overlap=CHUNK_OVERLAP
    )

    chunks: List[Chunk] = []
    for path in DOCS_DIR.rglob("*"):
        suf = path.suffix.lower()

        if suf == ".txt":
            text = path.read_text(encoding="utf-8", errors="ignore")
            for part in splitter.split_text(text):
                chunks.append(Chunk(
                    text=part,
                    meta={"source": str(path.relative_to(DOCS_DIR)), "page": None, "type": "txt"}
                ))

        elif suf == ".pdf":
            for page_no, page_text in read_pdf_pages(path):
                if not page_text.strip():
                    continue
                for part in splitter.split_text(page_text):
                    chunks.append(Chunk(
                        text=part,
                        meta={"source": str(path.relative_to(DOCS_DIR)), "page": page_no, "type": "pdf"}
                    ))
        else:
            continue

    if not chunks:
        raise RuntimeError(f"Aucun .txt ou .pdf trouvÃ© dans {DOCS_DIR}")
    return chunks

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Embeddings â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def embed(texts: List[str]) -> List[List[float]]:
    res = openai.embeddings.create(model=EMBED_MODEL, input=texts)
    return [d.embedding for d in res.data]

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Vector store â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def load_store() -> Tuple[faiss.Index, List[Chunk]]:
    index = faiss.read_index(str(FAISS_PATH))
    raw = json.loads(META_PATH.read_text(encoding="utf-8"))
    chunks = [Chunk(text=c["text"], meta=c["meta"]) for c in raw]
    return index, chunks

def save_store(index: faiss.Index, chunks: List[Chunk]) -> None:
    faiss.write_index(index, str(FAISS_PATH))
    META_PATH.write_text(
        json.dumps([{"text": c.text, "meta": c.meta} for c in chunks], ensure_ascii=False),
        encoding="utf-8"
    )

def get_faiss_store(chunks: List[Chunk]) -> Tuple[faiss.Index, List[Chunk]]:
    if FAISS_PATH.exists() and META_PATH.exists():
        print("âœ“  Loading existing vector store â€¦")
        return load_store()

    print("â³  Building vector store â€¦")
    all_vectors: List[List[float]] = []
    texts = [c.text for c in chunks]

    for i in tqdm(range(0, len(texts), 128), unit="batch"):
        all_vectors.extend(embed(texts[i:i+128]))

    mat = np.asarray(all_vectors, dtype=np.float32)
    index = faiss.IndexFlatL2(mat.shape[1])
    index.add(mat)

    save_store(index, chunks)
    return index, chunks

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Retrieval â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def retrieve(query: str, index: faiss.Index, chunks: List[Chunk], k: int = TOP_K) -> List[Chunk]:
    q_vec = np.asarray(embed([query])[0], dtype=np.float32).reshape(1, -1)
    _, idxs = index.search(q_vec, k)
    out: List[Chunk] = []
    for i in idxs[0]:
        if i < 0:
            continue
        out.append(chunks[int(i)])
    return out

def format_sources(ctx: List[Chunk]) -> str:
    """Return unique [source:page] list."""
    seen = set()
    refs = []
    for c in ctx:
        src = c.meta.get("source", "unknown")
        page = c.meta.get("page", None)
        ref = f"{src}:{page}" if page is not None else f"{src}"
        if ref not in seen:
            seen.add(ref)
            refs.append(ref)
    return " ; ".join(f"[{r}]" for r in refs[:8])

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Prompt helpers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def build_user_prompt(question: str, ctx_chunks: List[Chunk]) -> str:
    context_block = "\n\n".join(
        f"[Doc {i+1} | {c.meta.get('source')} | page {c.meta.get('page')}]\n{c.text}"
        for i, c in enumerate(ctx_chunks)
    )
    return (
        "Contexte (extraits de documents pÃ©dagogiques):\n"
        f"{context_block}\n\n"
        f"Question: {question}\n\n"
        "RÃ©ponds en t'appuyant uniquement sur le contexte. "
        "Si tu ne peux pas rÃ©pondre, dis Â« Je ne sais pas. Â»"
    )

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Chat loop â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def chat_loop(index: faiss.Index, chunks: List[Chunk]) -> None:
    history: List[Dict[str, str]] = []
    system_msg = {"role": "system", "content": SYSTEM_PROMPT}

    while True:
        try:
            q = input("\nğŸ’¬  Question (Ctrl-C pour quitter): ").strip()
        except KeyboardInterrupt:
            print("\nBye!")
            break

        if not q:
            continue

        ctx = retrieve(q, index, chunks)
        user_prompt = build_user_prompt(q, ctx)

        # Transparence: afficher le contexte rÃ©cupÃ©rÃ© (option pÃ©dagogique)
        print("\nğŸ”  Contexte rÃ©cupÃ©rÃ©:")
        print("â”€" * 60)
        for i, c in enumerate(ctx, 1):
            head = f"[Doc {i}] {c.meta.get('source')} | page {c.meta.get('page')}"
            print(head)
            print(textwrap.fill(c.text.replace("\n", " "), width=88))
            print("-" * 60)

        messages = [system_msg] + history + [{"role": "user", "content": user_prompt}]
        response = openai.chat.completions.create(
            model=CHAT_MODEL, messages=messages, temperature=0.2
        )
        answer = response.choices[0].message.content.strip()

        # Ajout automatique des sources si l'assistant n'en met pas
        srcs = format_sources(ctx)
        if answer and ("[" not in answer or "]" not in answer):
            answer = f"{answer}\n\nSources: {srcs}"

        print("ğŸ¤–  RÃ©ponse:\n")
        print(textwrap.fill(answer, width=88))

        # Historique lÃ©ger: question brute + rÃ©ponse (sans recoller les docs)
        history.extend([
            {"role": "user", "content": q},
            {"role": "assistant", "content": answer},
        ])

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ main â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
if __name__ == "__main__":
    chunks = load_and_split()
    index, chunks = get_faiss_store(chunks)
    chat_loop(index, chunks)
